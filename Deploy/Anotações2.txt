SETUP EM TODOS OS SERVERS
1- sudo yum update -y
3- sudo yum install -y yum-utils
4- sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

5- sudo yum install docker-ce docker-ce-cli containerd.io

6- sudo systemctl start docker (necessario para rodar o docker)

7- sudo docker run hello-world (para testar o docker)
-----------------------------------------------------------------------

NO MASTER:
1-sudo docker pull sdesilva26/spark_master:0.0.2
2-sudo docker swarm init
3-sudo docker network create -d overlay --attachable spark-net
4-sudo docker run -it --name spark-master --network spark-net -p 8080:8080 sdesilva26/spark_master:0.0.2

NOS WORKERS:
sudo systemctl start docker
1-docker pull sdesilva26/spark_worker:0.0.2
2-sudo docker run -it --name spark-worker1 --network spark-net -p 8081:8081 -e MEMORY=6G -e CORES=3 sdesilva26/spark_worker:0.0.2


NO SUBMIT:
1-docker pull sdesilva26/spark_submit:0.0.2
2-sudo docker run -it --name spark-submit --network spark-net -p 4040:4040 sdesilva26/spark_submit:0.0.2 bash

3-$SPARK_HOME/bin/pyspark --conf spark.executor.memory=5G --conf spark.executor.cores=3 --master spark://sparkdemo_spark-master:7077

  $SPARK_HOME/bin/pyspark --conf spark.executor.memory=5G --conf spark.executor.cores=3 --master spark://spark-master:7077
(SHEL INTERAIVO DISTRIBUIDO)

4-$SPARK_HOME/bin/spark-submit --conf spark.executor.cores=3 --conf spark.executor.memory=5G --master spark://spark-master:7077 $SPARK_HOME/examples/src/main/python/pi.py 20
(enviando jobs para o cluster)